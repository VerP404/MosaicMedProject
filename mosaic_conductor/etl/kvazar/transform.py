import json
from datetime import date
from dagster import asset, Field, String, OpExecutionContext, AssetIn
from config.settings import ORGANIZATIONS
from mosaic_conductor.etl.common.connect_db import connect_to_db


@asset(
    config_schema={
        "mapping_file": Field(String),
        "table_name": Field(String),
        "is_talon": Field(bool, default_value=False)
    },
    ins={"kvazar_extract": AssetIn()}
)
def kvazar_transform(context: OpExecutionContext, kvazar_extract: dict) -> dict:
    """
    –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:
      1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∏–∑ mapping.json, –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ—Ç —Å—Ç–æ–ª–±—Ü—ã —Å–æ–≥–ª–∞—Å–Ω–æ mapping_fields.
      2. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤.
      3. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç DataFrame —Ç–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏.
      4. –ï—Å–ª–∏ –≤ mapping.json –∑–∞–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –ø–æ–ª–µ "check_fields", —É–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö
         –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö.
      5. –ó–∞–ø–æ–ª–Ω—è–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ –ë–î –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º "-".
      6. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Ç–∞–ª–æ–Ω–æ–≤ (is_talon=True –∏–ª–∏ table_name –≤ —Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö),
         —Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–æ–ª–±–µ—Ü is_complex —Å –±—É–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º,
         –∞ –∑–∞—Ç–µ–º, –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ (talon, source), –¥–ª—è –≥—Ä—É–ø–ø —Å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å—å—é
         —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç is_complex = True.
      7. –ï—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ç–∞–ª–æ–Ω—ã, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è report_year –∏ report_month.
      8. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–∏–±–æ –µ–¥–∏–Ω—ã–π DataFrame, –ª–∏–±–æ —Å–ª–æ–≤–∞—Ä—å —Å –≤–µ—Ç–∫–∞–º–∏ "normal" –∏ "complex".
    """
    config = context.op_config
    mapping_file = config["mapping_file"]
    table_name = config["table_name"]

    df = kvazar_extract.get("data")
    if df is None:
        context.log.info("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏!")
        raise ValueError("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
    with open(mapping_file, "r", encoding="utf-8") as f:
        mappings = json.load(f)
    table_config = mappings.get("tables", {}).get(table_name, {})
    column_mapping = table_config.get("mapping_fields", {})

    expected_original_cols = list(column_mapping.keys())
    actual_cols = list(df.columns)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º CSV
    missing_in_csv = set(expected_original_cols) - set(actual_cols)
    if missing_in_csv:
        context.log.info(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã –≤ CSV: {missing_in_csv}")
        raise KeyError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ CSV: {missing_in_csv}")
    extra_in_csv = set(actual_cols) - set(expected_original_cols)
    if extra_in_csv:
        context.log.info(f"‚ö†Ô∏è –õ–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã –≤ CSV: {extra_in_csv}. –û–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")

    df = df.rename(columns=column_mapping)
    expected_cols = list(column_mapping.values())
    actual_transformed_cols = list(df.columns)
    missing_after_rename = set(expected_cols) - set(actual_transformed_cols)
    if missing_after_rename:
        context.log.info(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è: {missing_after_rename}")
        raise KeyError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è: {missing_after_rename}")
    extra_after_rename = set(actual_transformed_cols) - set(expected_cols)
    if extra_after_rename:
        context.log.info(f"‚ö†Ô∏è –õ–∏—à–Ω–∏–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è: {extra_after_rename}. –û–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
    df = df[expected_cols]

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –µ—Å–ª–∏ –≤ mapping –∑–∞–¥–∞–Ω–æ –ø–æ–ª–µ "check_fields", —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏,
    # –≥–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è.
    check_fields = table_config.get("check_fields", [])
    if check_fields:
        original_len = len(df)
        # –ó–¥–µ—Å—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –≤ check_fields —É–∫–∞–∑–∞–Ω—ã –∏–º–µ–Ω–Ω–æ –∏–º–µ–Ω–∞, –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è (new names)
        df = df.dropna(subset=check_fields)
        context.log.info(
            f"‚ö†Ô∏è –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–æ–∫: {original_len - len(df)} (—É–¥–∞–ª–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö: {check_fields})"
        )
    else:
        context.log.info("–ü–æ–ª–µ 'check_fields' –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äì –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –ø—É—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.")

    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ –ë–î –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º "-"
    engine, conn = connect_to_db(organization=ORGANIZATIONS, context=context)
    sql = f"""
          SELECT column_name 
          FROM information_schema.columns 
          WHERE table_name = '{table_name}' 
            AND data_type = 'character varying';
        """
    with conn.cursor() as cursor:
        cursor.execute(sql)
        db_columns = [row[0] for row in cursor.fetchall()]
    conn.close()
    if not db_columns:
        context.log.info(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}.")
        raise ValueError(f"‚ùå –ù–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}.")
    for col in db_columns:
        if col not in df.columns:
            context.log.info(f"‚ö†Ô∏è –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π —Å—Ç–æ–ª–±–µ—Ü '{col}' —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º '-' –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            df[col] = "-"

    # –æ—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    df = df.replace('`', '', regex=True)

    # –ï—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ç–∞–ª–æ–Ω—ã, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ is_complex –±—É–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º.
    if config.get("is_talon", False) or table_name in ["load_data_talons", "load_data_complex_talons"]:
        context.log.info("‚ÑπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç–∞–ª–æ–Ω–æ–≤ ‚Äì –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ is_complex –∫–∞–∫ boolean.")
        df["is_complex"] = False

        def compute_report_year(report_period, treatment_end):
            return treatment_end[-4:] if report_period == '-' else report_period[-4:]

        def compute_report_month(report_period, treatment_end):
            current_date = date.today()
            if report_period == '-':
                if current_date.day <= 4:
                    month_from_treatment = int(treatment_end[3:5]) if len(treatment_end) >= 6 else current_date.month
                    return current_date.month if month_from_treatment == current_date.month else (
                        12 if current_date.month == 1 else current_date.month - 1)
                else:
                    return current_date.month
            else:
                month_mapping = {
                    '–Ø–Ω–≤–∞—Ä—è': 1, '–§–µ–≤—Ä–∞–ª—è': 2, '–ú–∞—Ä—Ç–∞': 3, '–ê–ø—Ä–µ–ª—è': 4,
                    '–ú–∞—è': 5, '–ò—é–Ω—è': 6, '–ò—é–ª—è': 7, '–ê–≤–≥—É—Å—Ç–∞': 8,
                    '–°–µ–Ω—Ç—è–±—Ä—è': 9, '–û–∫—Ç—è–±—Ä—è': 10, '–ù–æ—è–±—Ä—è': 11, '–î–µ–∫–∞–±—Ä—è': 12
                }
                month_str = report_period.split()[0].strip()
                return month_mapping.get(month_str, None)

        df['report_year'] = df.apply(lambda row: compute_report_year(row['report_period'], row['treatment_end']),
                                     axis=1)
        df['report_month'] = df.apply(lambda row: compute_report_month(row['report_period'], row['treatment_end']),
                                      axis=1)

        grouped = df.groupby(["talon", "source"])
        for (talon, source), group in grouped:
            if len(group) > 1:
                df.loc[group.index, "is_complex"] = True
        df["is_complex"] = df["is_complex"].fillna(False).astype(bool)

        normal_df = df[df["is_complex"] == False].copy()
        complex_df = df[df["is_complex"] == True].copy()
        context.log.info(
            f"üîÑ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}. –ù–æ—Ä–º–∞–ª—å–Ω—ã—Ö: {len(normal_df)}, –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö: {len(complex_df)}."
        )
        return {
            "normal": {"table_name": "load_data_talons", "data": normal_df},
            "complex": {"table_name": "load_data_complex_talons", "data": complex_df}
        }
    else:
        context.log.info(f"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è {table_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
        return {"table_name": table_name, "data": df}
