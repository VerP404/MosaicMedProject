from abc import abstractmethod, ABC
from datetime import datetime

import fdb
import pandas as pd
from sqlalchemy import create_engine, text

from apps.data_loader.selenium.oms import selenium_oms, logger
from config.settings import DATABASES

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
postgres_settings = DATABASES['default']
engine = create_engine(
    f'postgresql://{postgres_settings["USER"]}:{postgres_settings["PASSWORD"]}@{postgres_settings["HOST"]}:{postgres_settings["PORT"]}/{postgres_settings["NAME"]}'
)


class BaseDataLoader(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è ETL-–ø—Ä–æ—Ü–µ—Å—Å–∞. –°–æ–¥–µ—Ä–∂–∏—Ç:
    - –û–±—â–∏–µ –ø–æ–ª—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: message, import_record_id, added_count –∏ —Ç.–¥.
    - –ú–µ—Ç–æ–¥—ã –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ DataImport: _create_initial_data_import_record, _update_data_import_record
    - run_etl(), –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç:
        1) _create_initial_data_import_record()
        2) extract()
        3) transform()
        4) load()
        5) —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
    - –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ transform() –∏ load() (—á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∏ MERGE).
    """

    def __init__(self,
                 engine,
                 table_name,
                 table_name_temp,
                 data_type_name,
                 column_mapping,
                 column_check,
                 columns_for_update,
                 file_format='csv',
                 sep=';',
                 dtype=str,
                 encoding='utf-8',
                 clear_all_rows=False):
        """
        :param engine: SQLAlchemy engine –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ
        :param table_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, data_loader_omsdata)
        :param table_name_temp: –ù–∞–∑–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, temp_oms_data)
        :param data_type_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö (OMS, –ö–≤–∞–∑–∞—Ä –∏ —Ç.–¥.) ‚Äî –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ DataImport
        :param column_mapping: dict { 'CSV_–∫–æ–ª–æ–Ω–∫–∞': 'DB_–ø–æ–ª–µ' } –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
        :param column_check: –ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ (—É–±–∏—Ä–∞–µ–º NaN)
        :param columns_for_update: –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –¥–µ–ª–∞–µ–º MERGE ON CONFLICT
        :param file_format: –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (csv, excel –∏ —Ç.–ø.) ‚Äî –µ—Å–ª–∏ –Ω–∞–¥–æ
        :param sep: –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è CSV
        :param dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é str)
        :param encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ (utf-8, cp1251 –∏ —Ç.–¥.)
        :param clear_all_rows: –ü—Ä–∏ True –º–æ–∂–µ–º –æ—á–∏—â–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """

        # -- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ --
        self.engine = engine
        self.table_name = table_name
        self.table_name_temp = table_name_temp
        self.data_type_name = data_type_name
        self.column_mapping = column_mapping
        self.column_check = column_check
        self.columns_for_update = columns_for_update
        self.file_format = file_format
        self.sep = sep
        self.dtype = dtype
        self.encoding = encoding
        self.clear_all_rows = clear_all_rows

        # -- –ü–æ–ª—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è / —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∫–ª–∞—Å—Å–∞) --
        self.message = ''
        self.import_record_id = None
        self.added_count = 0
        self.updated_count = 0
        self.error_count = 0

        # -- –°—á—ë—Ç—á–∏–∫–∏ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ --
        self.row_counts = {
            "before_processing": 0,
            "after_reading_csv": 0,
            "to_update": 0,
            "to_insert": 0,
            "after_processing": 0,
        }

    # =========================================================================
    # –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø—É—Å–∫–∞–µ—Ç ETL
    # =========================================================================
    def run_etl(self, source_name: str):
        """
        –û–±—â–∏–π ¬´—à–∞–±–ª–æ–Ω¬ª –ø—Ä–æ—Ü–µ—Å—Å–∞:
          1. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ DataImport (–Ω–∞—á–∞–ª–æ)
          2. extract() ‚Äì –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ (–¥–æ—á–µ—Ä–Ω–∏–µ –∫–ª–∞—Å—Å—ã –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç)
          3. transform() ‚Äì –æ–±—Ä–∞–±–æ—Ç–∫–∞
          4. load() ‚Äì –∑–∞–≥—Ä—É–∑–∫–∞ (—á–µ—Ä–µ–∑ temp + MERGE)
          5. —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        :param source_name: –ª–∏–±–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –ª–∏–±–æ —á—Ç–æ-—Ç–æ –µ—â—ë (URL, login, etc.)
        """
        start_time = datetime.now()
        self.message += f"[{start_time}] –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.\n"

        # 1) –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å DataImport (–ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)
        self._create_initial_data_import_record(source_name)

        # 2) EXTRACT (–¥–æ—á–µ—Ä–Ω–∏–µ –∫–ª–∞—Å—Å—ã –±—É–¥—É—Ç —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å)
        try:
            logger.info("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —ç—Ç–∞–ø EXTRACT")
            df = self.extract(source_name)
            self.row_counts["after_reading_csv"] = len(df)
            self.message += f"EXTRACT: –ø–æ–ª—É—á–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫.\n"

            # 3) TRANSFORM
            logger.info("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —ç—Ç–∞–ø TRANSFORM")
            df = self.transform(df)
            self.message += f"TRANSFORM: –æ—Å—Ç–∞–ª–æ—Å—å {len(df)} —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏.\n"

            # 4) LOAD
            logger.info("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —ç—Ç–∞–ø LOAD")
            self.load(df)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ETL: {e}", exc_info=True)
            self.error_count += 1
            self.message += f"–û–®–ò–ë–ö–ê: {e}\n"
            self._update_data_import_record()
            raise

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        end_time = datetime.now()
        elapsed = end_time - start_time
        self.message += f"–ì–æ—Ç–æ–≤–æ. –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed}\n"
        self._update_data_import_record()

    # =========================================================================
    # –ú–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∏–ª–∏ —É–∂–µ –≤ –±–∞–∑–æ–≤–æ–º –∫–ª–∞—Å—Å–µ
    # =========================================================================
    @abstractmethod
    def extract(self, source_name) -> pd.DataFrame:
        """
        –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö:
         - –µ—Å–ª–∏ CSV -> —á–∏—Ç–∞–µ–º CSV
         - –µ—Å–ª–∏ Selenium -> –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–æ–±–æ—Ç –∏ —á–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
         - –µ—Å–ª–∏ DB -> –∑–∞–ø—Ä–æ—Å –∫ –¥—Ä—É–≥–æ–π –ë–î
        """
        pass

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è DataFrame:
         - –≤—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ column_mapping
         - –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
         - dropna –ø–æ column_check
         - fillna('-'), —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
         - —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ columns_for_update
        """
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        df = df[list(self.column_mapping.keys())].rename(columns=self.column_mapping)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç column_check
        df.dropna(subset=[self.column_check], inplace=True)

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ
        df.fillna("-", inplace=True)
        df.replace('`', '', regex=True, inplace=True)
        df.replace('\u00A0', ' ', regex=True, inplace=True)

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–∞–º
        df = df.astype(str)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏ –¥–ª—è "–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ"
        if 'goal' in df.columns and 'talon' in df.columns:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ goal == "–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ"
            mask = df['goal'] == '–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ'
            stationary_df = df[mask]

            # –î–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã _1, _2 –∏ —Ç.–¥.
            stationary_df['talon'] = stationary_df.groupby('talon').cumcount().add(1).astype(str).radd(
                stationary_df['talon'] + '_')

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame
            df.loc[mask, 'talon'] = stationary_df['talon']

        # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏–Ω—ã –ø–æ–ª–µ–π (VARCHAR) –∏–∑ –ë–î
        column_max_lengths = self.get_column_max_lengths()

        # üîπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ max_length
        for column, max_length in column_max_lengths.items():
            if column in df.columns:
                df[column] = df[column].astype(str).str[:max_length]

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df.fillna("-", inplace=True)
        df.replace('`', '', regex=True, inplace=True)
        df.replace('\u00A0', ' ', regex=True, inplace=True)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        df.drop_duplicates(subset=self.columns_for_update, inplace=True)

        return df

    def load(self, df: pd.DataFrame):
        """
        –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏:
         - –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ "–¥–æ"
         - –°–æ–∑–¥–∞—Ç—å/–æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
         - to_sql(df) -> temp
         - —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã
         - –ø–æ—Å—á–∏—Ç–∞—Ç—å to_update/to_insert
         - MERGE
         - –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ "–ø–æ—Å–ª–µ"
        """
        # –ü–æ–¥—Å—á—ë—Ç "–¥–æ"
        with self.engine.connect() as conn:
            q = f"SELECT COUNT(*) FROM {self.table_name};"
            self.row_counts["before_processing"] = conn.execute(text(q)).scalar()
            self.message += f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.row_counts['before_processing']}\n"
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        with self.engine.connect() as conn:
            exists = conn.execute(text(f"""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.tables
                    WHERE table_name = '{self.table_name_temp}'
                )
            """)).scalar()
            if exists:
                conn.execute(text(f"TRUNCATE TABLE {self.table_name_temp}"))
            else:
                self.message += f"–¢–∞–±–ª–∏—Ü–∞ {self.table_name_temp} –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.\n"

        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        df.to_sql(self.table_name_temp, self.engine, if_exists="replace", index=False)

        # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å—ã
        self._create_index(self.table_name)
        self._create_index(self.table_name_temp)

        # –°—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –Ω–∞ update/insert
        with self.engine.connect() as conn:
            rows_to_update_query = f"""
            SELECT COUNT(*)
            FROM {self.table_name_temp} AS temp
            INNER JOIN {self.table_name} AS target
            ON {' AND '.join([f'temp.{col} = target.{col}' for col in self.columns_for_update])}
            """
            self.row_counts["to_update"] = conn.execute(text(rows_to_update_query)).scalar()
            rows_to_insert_query = f"""
            SELECT COUNT(*)
            FROM {self.table_name_temp} AS temp
            LEFT JOIN {self.table_name} AS target
            ON {' AND '.join([f'temp.{col} = target.{col}' for col in self.columns_for_update])}
            WHERE target.{self.columns_for_update[0]} IS NULL
            """
            self.row_counts["to_insert"] = conn.execute(text(rows_to_insert_query)).scalar()

        self.message += f"LOAD: –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è {self.row_counts['to_update']}, –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è {self.row_counts['to_insert']}.\n"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
        missing_columns = self._get_missing_columns()

        # –§–æ—Ä–º–∏—Ä—É–µ–º MERGE
        merge_query = self._build_merge_query(missing_columns)

        # –í—ã–ø–æ–ª–Ω—è–µ–º MERGE
        try:
            with self.engine.begin() as conn:
                conn.execute(text(merge_query))
            self.message += f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω—ã/–æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ {self.table_name}.\n"
        except Exception as e:
            self.error_count += 1
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ MERGE: {e}")

        # –ü–æ–¥—Å—á—ë—Ç "–ø–æ—Å–ª–µ"
        with self.engine.connect() as conn:
            after_count = conn.execute(text(q)).scalar()
            self.row_counts["after_processing"] = after_count
            self.message += f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {after_count}\n"
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫–∏
        self.added_count += self.row_counts["to_insert"]
        self.updated_count += self.row_counts["to_update"]

    # =========================================================================
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã (–∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∫–ª–∞—Å—Å–∞, –Ω–µ–º–Ω–æ–≥–æ –¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)
    # =========================================================================
    def _create_initial_data_import_record(self, source_name):
        """
        –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –≤ data_loader_dataimport (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ).
        """
        sql_query = """
        INSERT INTO data_loader_dataimport
        (csv_file, date_added, added_count, updated_count, error_count,
         data_type_id, message)
        VALUES (
          :csv_file,
          NOW(),
          0, 0, 0,
          (SELECT id FROM data_loader_datatype WHERE name = :data_type_name),
          :message
        )
        RETURNING id
        """
        with self.engine.begin() as connection:
            result = connection.execute(
                text(sql_query),
                {
                    'csv_file': source_name[:100],
                    'data_type_name': self.data_type_name,
                    'message': '–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏.'
                }
            )
            self.import_record_id = result.fetchone()[0]

    def _update_data_import_record(self):
        """
        –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ data_loader_dataimport (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ).
        """
        if not self.import_record_id:
            return
        sql_update = """
        UPDATE data_loader_dataimport
        SET added_count = :added_count,
            updated_count = :updated_count,
            error_count = :error_count,
            message = :message
        WHERE id = :import_record_id
        """
        with self.engine.begin() as connection:
            connection.execute(text(sql_update), {
                'added_count': self.added_count,
                'updated_count': self.updated_count,
                'error_count': self.error_count,
                'message': self.message,
                'import_record_id': self.import_record_id
            })

    def _create_index(self, table_name: str):
        """
        –°–æ–∑–¥–∞—ë—Ç –∏–Ω–¥–µ–∫—Å –ø–æ self.columns_for_update, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        """
        index_name = f"idx_{table_name}_update"
        create_index_query = f"""
        DO $$ BEGIN
            IF NOT EXISTS (
                SELECT 1 
                FROM pg_indexes 
                WHERE schemaname = 'public' 
                  AND tablename = '{table_name}' 
                  AND indexname = '{index_name}'
            ) THEN
                CREATE INDEX {index_name}
                ON {table_name} ({', '.join(self.columns_for_update)});
            END IF;
        END $$;
        """
        with self.engine.connect() as conn:
            conn.execute(text(create_index_query))

    def _get_missing_columns(self):
        """
        –í—ã—è—Å–Ω—è–µ—Ç, –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –µ—Å—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ, –Ω–æ –Ω–µ—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π.
        """
        with self.engine.connect() as conn:
            main_cols_q = f"SELECT column_name FROM information_schema.columns WHERE table_name = '{self.table_name}'"
            main_cols = [r[0] for r in conn.execute(text(main_cols_q)).fetchall()]

            temp_cols_q = f"SELECT column_name FROM information_schema.columns WHERE table_name = '{self.table_name_temp}'"
            temp_cols = [r[0] for r in conn.execute(text(temp_cols_q)).fetchall()]

        missing = [c for c in main_cols if (c not in temp_cols and c != 'id')]
        return missing

    def _build_merge_query(self, missing_columns):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å MERGE:
        INSERT INTO table_name (...)
        SELECT ...
        FROM table_name_temp
        ON CONFLICT (columns_for_update)
        DO UPDATE SET ...
        """
        # –í—Å–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç–∞–≤–ª—è–µ–º
        insert_cols = list(self.column_mapping.values()) + missing_columns
        insert_cols_sql = ", ".join(insert_cols)

        # –ß–∞—Å—Ç—å SELECT
        select_cols_sql = ", ".join(self.column_mapping.values())
        if missing_columns:
            missing_sql = ", ".join([f"'-' AS {c}" for c in missing_columns])
            select_all_for_insert = f"{select_cols_sql}, {missing_sql}"
        else:
            select_all_for_insert = select_cols_sql

        # ON CONFLICT
        conflict_sql = ", ".join(self.columns_for_update)
        print(self.columns_for_update)

        # DO UPDATE
        update_parts = []
        for col in self.column_mapping.values():
            if col not in self.columns_for_update:
                update_parts.append(f"{col} = EXCLUDED.{col}")
        for col in missing_columns:
            update_parts.append(f"{col} = COALESCE(EXCLUDED.{col}, '-')")

        update_sql = ", ".join(update_parts)

        merge_query = f"""
        INSERT INTO {self.table_name} ({insert_cols_sql})
        SELECT {select_all_for_insert}
        FROM {self.table_name_temp}
        ON CONFLICT ({conflict_sql})
        DO UPDATE SET {update_sql};
        """
        # –£–¥–∞–ª–∏–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        merge_query = " ".join(merge_query.split())
        return merge_query

    def get_column_max_lengths(self):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã (VARCHAR(N)) –¥–ª—è –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ PostgreSQL.
        """
        query = f"""
        SELECT column_name, character_maximum_length
        FROM information_schema.columns
        WHERE table_name = '{self.table_name}' AND data_type = 'character varying'
        """

        with self.engine.connect() as conn:
            result = conn.execute(text(query)).fetchall()

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å { 'column_name': max_length }
        return {row[0]: row[1] for row in result if row[1] is not None}


class CsvDataLoader(BaseDataLoader):
    """
    –ù–∞—Å–ª–µ–¥–Ω–∏–∫ BaseDataLoader, –≥–¥–µ –º–µ—Ç–æ–¥ extract()
    —á–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ (source_name).
    """

    def extract(self, source_name) -> pd.DataFrame:
        if self.file_format.lower() == 'csv':
            df = pd.read_csv(
                source_name,
                sep=self.sep,
                dtype=self.dtype,
                encoding=self.encoding,
                na_values="-",
                low_memory=False
            )
            # –£–±–∏—Ä–∞–µ–º Unnamed-—Å—Ç–æ–ª–±—Ü—ã, –µ—Å–ª–∏ –µ—Å—Ç—å
            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
            return df
        elif self.file_format.lower() == 'excel':
            return pd.read_excel(source_name)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {self.file_format}")


class SeleniumDataLoader(BaseDataLoader):
    """
    –ù–∞—Å–ª–µ–¥–Ω–∏–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º extract() —Å–Ω–∞—á–∞–ª–∞ –≤—ã–∑—ã–≤–∞–µ—Ç selenium_oms,
    –∞ –ø–æ—Ç–æ–º —á–∏—Ç–∞–µ—Ç —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –∫–∞–∫ CSV.
    """

    def __init__(self, engine, table_name, table_name_temp,
                 data_type_name, column_mapping, column_check, columns_for_update,
                 username, password, start_date, end_date, start_date_treatment,
                 file_format='csv', sep=';', dtype=str, encoding='utf-8',
                 clear_all_rows=False, browser='firefox'):
        super().__init__(
            engine=engine,
            table_name=table_name,
            table_name_temp=table_name_temp,
            data_type_name=data_type_name,
            column_mapping=column_mapping,
            column_check=column_check,
            columns_for_update=columns_for_update,
            file_format=file_format,
            sep=sep,
            dtype=dtype,
            encoding=encoding,
            clear_all_rows=clear_all_rows

        )
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è selenium
        self.username = username
        self.password = password
        self.start_date = start_date
        self.end_date = end_date
        self.start_date_treatment = start_date_treatment
        self.browser = browser

    def extract(self, source_name) -> pd.DataFrame:
        """
        1) –í—ã–∑—ã–≤–∞–µ–º selenium_oms, –ø–æ–ª—É—á–∞–µ–º success, file_path
        2) –ï—Å–ª–∏ success, —á–∏—Ç–∞–µ–º CSV-—Ñ–∞–π–ª
        3) –í–æ–∑–≤—Ä–∞—â–∞–µ–º DataFrame
        """
        success, downloaded_path = selenium_oms(
            self.username,
            self.password,
            self.start_date,
            self.end_date,
            self.start_date_treatment,
            self.browser
        )
        logger.info("Success: %s, Downloaded path: %s", success, downloaded_path)

        if not success or not downloaded_path:
            raise ValueError("Selenium –Ω–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª / –ø—É—Ç—å –ø—É—Å—Ç.")

        # –¢–µ–ø–µ—Ä—å —á–∏—Ç–∞–µ–º CSV/Excel –ø–æ –ª–æ–≥–∏–∫–µ, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π CsvDataLoader
        if self.file_format.lower() == 'csv':
            df = pd.read_csv(
                downloaded_path,
                sep=self.sep,
                dtype=self.dtype,
                encoding=self.encoding,
                na_values="-",
                low_memory=False
            )
            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
            return df
        elif self.file_format.lower() == 'excel':
            return pd.read_excel(downloaded_path)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {self.file_format}")


class FirebirdDataLoader(BaseDataLoader):
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Firebird.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ Firebird –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL.
    """

    def __init__(self, engine, table_name, table_name_temp, data_type_name, column_mapping, column_check,
                 columns_for_update,
                 firebird_dsn, firebird_user, firebird_password, firebird_charset='WIN1251', firebird_port=3050,
                 file_format='db', sep=';', dtype=str, encoding='utf-8', clear_all_rows=False):
        super().__init__(engine, table_name, table_name_temp, data_type_name, column_mapping, column_check,
                         columns_for_update,
                         file_format, sep, dtype, encoding, clear_all_rows)
        # Firebird-specific connection details
        self.firebird_dsn = firebird_dsn
        self.firebird_user = firebird_user
        self.firebird_password = firebird_password
        self.firebird_charset = firebird_charset
        self.firebird_port = firebird_port

    def extract(self, query: str) -> pd.DataFrame:
        """
        –ú–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ Firebird.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç SQL-–∑–∞–ø—Ä–æ—Å –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ DataFrame.
        :param query: SQL-–∑–∞–ø—Ä–æ—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        :return: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–ø—Ä–æ—Å–∞
        """
        try:
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ Firebird
            conn = fdb.connect(
                dsn=self.firebird_dsn,
                user=self.firebird_user,
                password=self.firebird_password,
                charset=self.firebird_charset,
                port=self.firebird_port
            )
            cursor = conn.cursor()

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            cursor.execute(query)
            data = cursor.fetchall()

            # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ –∫—É—Ä—Å–æ—Ä–∞
            columns = [desc[0] for desc in cursor.description]

            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            conn.close()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
            df = pd.DataFrame(data, columns=columns)
            return df

        except Exception as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Firebird –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
